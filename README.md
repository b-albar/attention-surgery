
<div align="center">

# Attention Surgery - WIP

[![License: Apache 2.0](https://img.shields.io/badge/License-Apache--2.0-green.svg)](https://opensource.org/licenses/Apache-2.0)
</div>

This project aims to explore alternative attention mechanisms to the standard softmax-based attention used in LLMs. Attention surgery refers to the process of replacing only the attention layers in a pre-trained LLM and re-training the attention layers to mimic the original model.

The goal is to obtain more efficent models while reaching close to the same performance as the original model.

## Supported models

- Llama 3.1

## Supported methods

- Sigmoid attention (no flash sigmoid attention yet)

## TODOs

- Everything
